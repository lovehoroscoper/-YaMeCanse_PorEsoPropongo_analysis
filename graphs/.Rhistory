qq + geom_smooth(method = "lm", formula = y~x)
cutWage <- cut2(training$wage, g = 3)
table(cutWage)
install.packages("Hmisc")
library(Hmisc)
cutWage <- cut2(training$wage, g = 3)
table(cutWage)
cutWage <- cut2(training$wage, g = 3)
table(cutWage)
p1 <- qplot(cutWage, age, data=training, fill=cutWage,
geom=c("boxplot", "jitter"))
grid.arrange(p1,p2,ncol=2)
library(ISLR)
library(ggplot2)
library(caret)
library(Hmisc)
p1 <- qplot(cutWage, age, data=training, fill=cutWage,
geom=c("boxplot", "jitter"))
grid.arrange(p1,p2,ncol=2)
p1 <- qplot(cutWage, age, data=training, fill=cutWage,
geom=c("boxplot", "jitter"))
grid.arrange(p1,p2,ncol=2)
p1 <- qplot(cutWage, age, data=training, fill=cutWage,
geom=c("boxplot"))
p2 <- qplot(cutWage, age, data=training, fill=cutWage,
geom=c("boxplot", "jitter"))
grid.arrange(p1,p2,ncol=2)
p1
p2
install.packages("latticeExtra")
install.packages("latticeExtra")
grid.arrange(p1,p2,ncol=2)
library(ISLR)
library(ggplot2)
library(caret)
library(Hmisc)
grid.arrange(p1,p2,ncol=2)
library(latticeExtra)
grid.arrange(p1,p2,ncol=2)
library(ISLR)
library(ggplot2)
library(caret)
library(Hmisc)
library(latticeExtra)
data(Wage)
summary(Wage)
inTrain <- createDataPartition(y=Wage$wage,
p = 0.7, list = FALSE)
training <-Wage[inTrain, ]
testing  <-Wage[-inTrain, ]
dim(training)
dim(testing)
# some visualizations
featurePlot(x=training[,c("age","education","jobclass")],
y = training$wage,
plot = "pairs")     # plots all features of data from caret
qplot(age, wage, data=training)
qplot(age, wage, colour=jobclass,
data=training)
qq <- qplot(age, wage, colour = education, data = training)
qq + geom_smooth(method = "lm", formula = y~x)
cutWage <- cut2(training$wage, g = 3)
table(cutWage)
p1 <- qplot(cutWage, age, data=training, fill=cutWage,
geom=c("boxplot"))
p2 <- qplot(cutWage, age, data=training, fill=cutWage,
geom=c("boxplot", "jitter"))
grid.arrange(p1,p2,ncol=2)
library(gridExtra)
install.packages("gridExtra")
library(gridExtra)
grid.arrange(p1,p2,ncol=2)
t1 <- table(cutWage, training$jobclass)
t1
prop.table(t1,1)
qplot(wage, colour = education, data=training,
geom = "density")
library(caret)
library(kernlab)
data(spam)
inTrain <- createDataPartition(y=spam$type,
p = 0.75, list = FALSE)
training <- spam[inTrain, ]
testing  <- spam[-inTrain, ]
hist(training$capitalAve, main = "", xlab = "ave. capital run length")
mean(training$capitalAve)
sd(training$capitalAve)
trainCapAve  <- training$capitalAve
trainCapAveS <- (trainCapAve - mean(trainCapAve))/sd(trainCapAve)
mean(trainCapAveS)
sd(trainCapAveS)
testCapAve  <- testing$capitalAve
testCapAveS <- (testCapAve - mean(trainCapAve))/sd(trainCapAve)
mean(testCapAveS)
sd(testCapAveS)
preObj <- preProcess(training[,-58], method = c("center", "scale"))
trainCapAveS <- predict(preObj, training[,-58])$capitalAve
mean(trainCapAveS)
sd(trainCapAveS)
testCapAveS <- predict(preObj, testing[,-58])$capitalAve
mean(testCapAveS)
sd(testCapAveS)
set.seed(32343)
modelFit <- train(type ~., data = training,
preProcess= c("center", "scale"),
method = "glm")
modelFit
preObj <- preProcess(training[,-58], method = c("BoxCox"))
trainCapAveS <- predict(preObj, training[,-58])$capitalAve
par(mfrow = c(1,2))
hist(trainCapAveS)
qqnorm(trainCapAveS)
set.seed(13343)
training$capAve <- training$capitalAve
selectNA <- rbinom(dim(training)[1], size = 1, prob = 0.05) == 1
training$capAve[selectNA] <- NA    # simulates missing values
preObj <- preProcess(training[,-58], method = "knnImpute") # imputes values
capAve <- predict(preObj, training[,-58]$capAve)
capAveTruth <- training$capitalAve
capAveTruth <- (capAveTruth-mean(capAveTruth))/sd(capAveTruth)
preObj <- preProcess(training[,-58], method = "knnImpute") # imputes values
capAve <- predict(preObj, training[,-58])$capAve
instal.packages("RANN")
install.packages("RANN")
capAve <- predict(preObj, training[,-58])$capAve
capAveTruth <- training$capitalAve
capAveTruth <- (capAveTruth-mean(capAveTruth))/sd(capAveTruth)
# imputing data
set.seed(13343)
training$capAve <- training$capitalAve
selectNA <- rbinom(dim(training)[1], size = 1, prob = 0.05) == 1
training$capAve[selectNA] <- NA    # simulates missing values
preObj <- preProcess(training[,-58], method = "knnImpute") # imputes values
capAve <- predict(preObj, training[,-58])$capAve
capAveTruth <- training$capitalAve
capAveTruth <- (capAveTruth-mean(capAveTruth))/sd(capAveTruth)
quantile(capAve - capAveTruth)
quantile((capAve - capAveTruth)[selectNA])
quantile((capAve - capAveTruth)[!selectNA])
library(kernlab)
data(spam)
spam$capitalAveSq <- spam$capitalAve^2
head(spam$capitalAveSq)
library(ISLR)
library(caret)
data(Wage)
inTrain <- createDataPartition(y=Wage$wage,
p = 0.7, list = FALSE)
training <-Wage[inTrain, ]
testing  <-Wage[-inTrain, ]
# creating dummy variables for categorical variables
table(training$jobclass)
dummies <- dummyVars(wage ~ jobclass, data = training)
head(predict(dummies, newdata = training))
# removing zero covariates
nsv <- nearZeroVar(training, saveMetrics=TRUE)
nsv
library(splines)
bsBasis <- bs(training$age, df=3)
bsBasis
lm1 <- lm(wage ~ bsBasis, data=training)
plot(training$age, training$wage, pch=19, cex=0.5)
points(training$age, predict(lm1, newdata= training),
col="red", pch=19, cex=0.5)
predict(bsBasis, age=testing$age)
library(caret)
library(kernlab)
data(spam)
inTrain <- createDataPartition(y=spam$type,
p = 0.75, list = FALSE)
training <-spam[inTrain, ]
testing  <-spam[-inTrain, ]
M <- abs(cor(training[,-58]))
diag(M) <- 0
which(M>0.8, arr.ind = T)
names(spam)[c(34,32)]
names(spam)[c(34,32)]
plot(spam[,34], spam[,32])
X <- 0.71*training$num415 + 0.71*training$num857
Y <- 0.71*training$num415 - 0.71*training$num857
plot(X,Y)
smallSpam <- spam[,c(34,32)]
prComp <- prcomp(smallSpam)
plot(prComp$x[,1], prComp$x[,2])
prComp$rotation
typeColor <- ((spam$type=="spam")* 1 + 1)
prComp <- prcomp(log10(spam[,58]+1))
plot(prComp$x[,1], prComp$x[,2], col=typeColor, xlab="PC1", ylab="PC2")
typeColor <- ((spam$type=="spam")* 1 + 1)
prComp <- prcomp(log10(spam[,-58]+1))
plot(prComp$x[,1], prComp$x[,2], col=typeColor, xlab="PC1", ylab="PC2")
preProc <- preProcess(log10(spam[,-58]+1), method="pca", pcaComp=2)
spamPC <- predict(preProc, log10(spam[,-58]+1))
plot(spamPC[,1],spamPC[,2],col=typeColor)
modelFit <- train(training$type~., method="glm",data=trainPC)
trainPC <- predict(preProc, log10(spam[,-58]+1))
modelFit <- train(training$type~., method="glm",data=trainPC)
preProc <- preProcess(log10(training[,-58]+1), method="pca", pcaComp=2)
trainPC <- predict(preProc, log10(spam[,-58]+1))
modelFit <- train(training$type~., method="glm",data=trainPC)
preProc <- preProcess(log10(training[,-58]+1), method="pca", pcaComp=2)
trainPC <- predict(preProc, log10(training[,-58]+1))
modelFit <- train(training$type~., method="glm",data=trainPC)
testPC <- predict(preProc, log10(testing[,-58]+1))
confusionMatrix(testing$type, predict(modelFit,textPC))
testPC <- predict(preProc, log10(testing[,-58]+1))
confusionMatrix(testing$type, predict(modelFit,testPC))
modelFit <- train(training$type ~., method="glm",
preProcess="pca", data=training)
confusionMatrix(testing$type, predict(modelFit, testing))
library(caret)
data(faithful)
set.seed(333)
inTrain <- createDataPartition(y=faithful$waiting,
p=0.5, list= FALSE)
trainFaith <- faithful[inTrain]
testFaith <- faitful[-inTrain,]
head(trainFaith)
testFaith <- faithful[-inTrain,]
head(trainFaith)
plot(trainFaith$waiting, trainFaith$eruptions, pch=19, col="blue", xlab= "Waiting", ylab = "Duration")
library(caret)
data(faithful)
set.seed(333)
inTrain <- createDataPartition(y=faithful$waiting,
p=0.5, list= FALSE)
trainFaith <- faithful[inTrain,]
testFaith <- faithful[-inTrain,]
head(trainFaith)
plot(trainFaith$waiting, trainFaith$eruptions,
pch=19, col="blue", xlab= "Waiting", ylab = "Duration")
lm1 <- lm(eruptions ~ waiting, data=trainFaith)
summary(lm1)
plot(trainFaith$waiting, trainFaith$eruptions,
pch=19, col="blue", xlab= "Waiting", ylab = "Duration")
lines(trainFaith$waiting, lm1$fitted, lwd=3)
coef(lm1)[1]+coef(lm1)[2]*80
newdata <- data.frame(waiting=80)
predict(lm1, newdata)
par(mfrow=c(1,2))
plot(trainFaith$waiting, trainFaith$eruptions,
pch=19, col="blue", xlab= "Waiting", ylab = "Duration")
lines(trainFaith$waiting, lm1$fitted, lwd=3)
plot(testFaith$waiting, testFaith$eruptions,
pch=19, col="blue", xlab= "Waiting", ylab = "Duration")
lines(testFaith$waiting, predict(lm1, newdata=testFaith) lwd=3)
par(mfrow=c(1,2))
plot(trainFaith$waiting, trainFaith$eruptions,
pch=19, col="blue", xlab= "Waiting", ylab = "Duration")
lines(trainFaith$waiting, lm1$fitted, lwd=3)
plot(testFaith$waiting, testFaith$eruptions,
pch=19, col="blue", xlab= "Waiting", ylab = "Duration")
lines(testFaith$waiting, predict(lm1, newdata=testFaith), lwd=3)
sqrt(sum(lm1$fitted-trainFaith$eruptions)^2)
sqrt(sum(lm1$fitted-trainFaith$eruptions)^2))
sqrt(sum((lm1$fitted-trainFaith$eruptions)^2))
sqrt(sum((lm1$fitted-trainFaith$eruptions)^2)) # RMSE on training
sqrt(sum((predict(lm1, newdata=testFaith)-trainFaith$eruptions)^2)) # RMSE on test
sqrt(sum((predict(lm1, newdata=testFaith)-testFaith$eruptions)^2)) # RMSE on test
pred1 <- predict(lm1, newdata=testFaith,
interval = "prediction")
ord <- order(testFaith$waiting)
plot(testFaith$waiting, testFaith$eruptions,
pch=19, col="blue", xlab= "Waiting", ylab = "Duration")
matlines(testFaith$waiting[ord],pred1[ord1,],type="l",,
col=c(1,2,2), lty=c(1,1,1), lwd=3)
pred1 <- predict(lm1, newdata=testFaith,
interval = "prediction")
ord <- order(testFaith$waiting)
plot(testFaith$waiting, testFaith$eruptions,
pch=19, col="blue", xlab= "Waiting", ylab = "Duration")
matlines(testFaith$waiting[ord],pred1[ord,],type="l",,
col=c(1,2,2), lty=c(1,1,1), lwd=3)
modFit <-train(eruptions~ waiting, data =trainFaith, method="lm")
summatu(modFit$finalModel)
summary(modFit$finalModel)
library(ISLR)
library(ggplot2)
library(caret)
data(Wage)
Wage <- subset(Wage, select=-c(logwage))
summary(Wage)
Wage <- subset(Wage, select=-c(logwage))
summary(Wage)
inTrain <- createDataPartition(y=Wage$wage,
p = 0.7, list = FALSE)
training <- Wage[inTrain,]
testing <- Wage[-inTrain, ]
dim(training)
dim(testing)
featurePlot(x=training[,c("age","education","jobclass")],
y =training$wage,
plot="pairs")
qpot(age, wage, data=training)
qplot(age, wage, data=training)
qplot(age, wage, colour=jobclas,data=training)
qplot(age, wage, colour=jobclass,data=training)
qplot(age, wage, colour=education,data=training)
modFit <- train(wage ~age + jobclass + education,
method ="lm", data=training)
finMod <- modFit$finalModel
print(modFit)
plot(finMod, 1, pch=19, cex=0.5, col="#00000010")
qplot(finMod$fitted,finMod$residuals, colour=race, data=training)
plot(finMod$residuals, pch=19)
pred <- predict(modFit, testing)
qplot(wage, pred, colour=year, data=testing)
modFitALL <- train(wage ~ ., data=training, method ="lm" )
pred <- predict(modFitAll, testing)
qplot(wage, pred, data=testing)
modFitAll <- train(wage ~ ., data=training, method ="lm" )
pred <- predict(modFitAll, testing)
qplot(wage, pred, data=testing)
q()
library(AppliedPredictiveModeling)
library(caret)
data(AlzheimerDisease)
install.packages(AppliedPredictiveModeling)
install.packages("AppliedPredictiveModeling")
library(AppliedPredictiveModeling)
library(caret)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
trainIndex = createDataPartition(diagnosis, p = 0.50)
training = adData[trainIndex,]
testing = adData[-trainIndex,]
training = adData[trainIndex, ]
adData = data.frame(diagnosis,predictors)
testIndex = createDataPartition(diagnosis, p = 0.50,list=FALSE)
training = adData[-testIndex,]
testing = adData[testIndex,]
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
names(data)
names(concrete)
View(concrete)
names <- colnames(concrete)
names <- names[-length(names)]
featurePlot(x = training[, names], y = training$CompressiveStrength, plot = "pairs")
names
names <- colnames(concrete)
names
index <- seq_along(1:nrow(training))   # creates a sequential index
ggplot(data = training, aes(x = index, y = CompressiveStrength)) + geom_point() +
theme_bw()
cutCS <- cut2(training$CompressiveStrength, g = 4)
summary(cutCS)
library(Hmisc)
cutCS <- cut2(training$CompressiveStrength, g = 4)
summary(cutCS)
ggplot(data = training, aes(y = index, x = cutCS)) + geom_boxplot() + geom_jitter(col = "blue") +
theme_bw()
featurePlot(x = training[, names], y = cutCS, plot = "box")
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
hist(concrete$SuperPlasticizer)
hist(concrete$Superplasticizer)
hist(log(concrete$Superplasticizer))
hist(log(concrete$Superplasticizer+1))
hist(log(concrete$Superplasticizer))
summary(concrete$Superplasticizer)
print log(0)
log(0)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
subset = training[,grep("^IL", names(training))]
preProcess(subset, thresh = 0.8, method = "pca")$numComp
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
trainSubset = training[,grep("^IL", names(training))]
testSubset = testing[,grep("^IL", names(testing))]
pp = preProcess(trainSubset, thresh = 0.8, method = "pca")
trainTransformed <- predict(pp, trainSubset)
testTransformed <- predict(pp, testSubset)
trainSubset$diagnosis = training$diagnosis
testSubset$diagnosis = testing$diagnosis
trainTransformed$diagnosis = training$diagnosis
testTransformed$diagnosis = testing$diagnosis
glmpca = train(diagnosis ~ ., data = trainTransformed, method = "glm")
glm = train(diagnosis ~ ., data = trainSubset, method = "glm")
round(confusionMatrix(testSubset$diagnosis,predict(glm, testSubset))$overall["Accuracy"],2)
round(confusionMatrix(testTransformed$diagnosis,predict(glmpca, testTransformed))$overall["Accuracy"],2)
rm(list = ls())
getStemLanguages()
library(Rstem)
install.packages(Rstem)
library(SnowballC)
getStemLanguages()
install.packages("wordnet")
tt <- readPDF(PdftotextOptions="-layout")
library(tm)
tt <- readPDF(PdftotextOptions="-layout")
rr <- tt(elem=list(uri="AAWE_WP16.pdf"),language="en",id="id1")
tt <- readPDF(control = list(text ="-layout"))
rr <- tt(elem=list(uri=dis),language="en",id="id1")
dis <- "~/Dropbox/NYU/Dissertation/SubmittedDissertation/Dissertation_FINAL.pdf"
dis
rr <- tt(elem=list(uri=dis),language="en",id="id1")
install.packages("Rpoppler")
install.packages("rstan")
smallClouds <- function(word) {
correlates <- as.data.frame(findAssocs(frequencies, word, 0.3))   # gets list of correlated words
newrow = c(1)
correlates = rbind(correlates, newrow)
rownames(correlates)[nrow(correlates)] <- c(word)                 # adds main word to dataframe
names(correlates) <- c("corr")
idx <- merge(correlates, mat, by = "row.names")                   # gets subset corr/freqs
setwd("~/Documents/#YaMeCanse_postales/graphs")
pdf(paste(word,".pdf", sep = ""))
wordcloud(idx$Row.names,idx$freq,
scale=c(3,0.5), max.words=50, random.order=FALSE,
rot.per=0.35, use.r.layout=FALSE,
colors=brewer.pal(8, "Dark2"))                          # creates wordcloud
dev.off()
}
library(foreign)
library(data.table)
library(tm)
library(wordcloud)
setwd("~/Documents/#YaMeCanse_postales")
getwd()
data <- as.data.table(read.delim(
"data/Postcard Dashboard  Orders - Movement Postcards_converted (utf16).txt",
header=TRUE, sep="\t", na.strings = "NA", fileEncoding = "UTF-16"))
names(data)                                 # verifies all variables are loaded properly
length(data$ID)                             # verifies that no observations were lost
data[, unique.id:=.GRP, by=data$Customer]   # creates unique user identifier
proposals <- data$Body[!is.na(data$Body)]
text <- VCorpus(VectorSource(proposals), readerControl= list(language = "spanish"))
inspect(text[1:10])
words = c(stopwords("spanish"), "#", "yamecanse", "yamecansé", "yamecansepor", "poresopropongo",
"yamecanseporesopropongo", "yamecanséporesopropongo", "por", "eso", "propongo", "que",
"canse", "cansé", "ser", "así", "cada", "mas", "solo", "cualquier", "sólo", "etc",
"yamecansè", "yamecansépor", "tan", "yame", "yamecanséde", "yamecansédetener",
"yamécansépor", "yamecanséyporesopropongo", "yamecase", "yopropongoquien")
text <- tm_map(text, content_transformer(tolower), mc.cores=1)
text <- tm_map(text, removeNumbers)
text <- tm_map(text, removePunctuation)
text <- tm_map(text, removeWords, words, lazy=TRUE)
text <- tm_map(text, stripWhitespace)
inspect(text[1:10])
tdm <- TermDocumentMatrix(text)          # creates matrix
removeSparseTerms(tdm, 0.4)              # removes sparse terms
FreqTerms <- findFreqTerms(tdm, 50)      # identify most common words in dataset
frequencies <- rowSums(as.matrix(tdm))
names(frequencies) <- rownames(as.matrix(tdm))
frequencies <- sort(frequencies, decreasing=T)
frequencies <- as.data.frame(frequencies)
names(frequencies) <- c("freq")
smallClouds <- function(word) {
correlates <- as.data.frame(findAssocs(tdm, word, 0.3))   # gets list of correlated words
newrow = c(1)
correlates = rbind(correlates, newrow)
rownames(correlates)[nrow(correlates)] <- c(word)                 # adds main word to dataframe
names(correlates) <- c("corr")
idx <- merge(correlates, frequencies, by = "row.names")           # gets subset corr/freqs
setwd("~/Documents/#YaMeCanse_postales/graphs")
pdf(paste(word,".pdf", sep = ""))
wordcloud(idx$Row.names,idx$freq,
scale=c(3,0.5), max.words=50, random.order=FALSE,
rot.per=0.35, use.r.layout=FALSE,
colors=brewer.pal(8, "Dark2"))                          # creates wordcloud
dev.off()
}
wordcloud(text, scale=c(3,0.01), max.words=100, random.order=FALSE,
rot.per=0.35, use.r.layout=FALSE,
colors=brewer.pal(8, "Dark2"))
for (i in FreqTerms) {
smallClouds(i)
}
dissimilarity(tdm, method = "cosine")
dissimilarity(tdm, method = "cosine")
cluster <- hclust(dist(tdm), method = "ward")
library(proxy)
install.packages("proxy")
library(proxy)
dist(tdm, method = "cosine")
cluster <- hclust(dist(tdm), method = "ward.D")
kmeans <- kmeans(tdm, 2)
summary(kmeans)
kmeans
kmeans$cluster
head(kmeans$cluster)
FreqTerms
syns_salario <- c("salarios", "sueldo", "sueldos")
findFreqTerms(tdm, 100)
findFreqTerms(tdm, 30)
text <- tm_map(text, replaceWords, "politicos", by = "políticos")
text <- tm_map(text, replaceWords, "politicos", by = "políticos", mc.cores=1)
rm(list=ls(all=TRUE))
